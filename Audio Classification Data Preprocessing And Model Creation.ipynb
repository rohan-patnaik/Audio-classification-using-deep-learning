{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'UrbanSound8K/UrbanSound8K/audio/fold1/7383-3-0-0.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "data, sample_rate = librosa.load(filename)\n",
    "print(\"Sample rate :\", sample_rate)\n",
    "librosa.display.waveshow(data, sr=sample_rate)\n",
    "ipd.Audio(data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same audio EDA as above but with sci-py.(1 major disadvantage here is the data collected from audio is not regularized)\n",
    "from scipy.io import wavfile as wav\n",
    "wav_sample_rate, wav_data = wav.read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "audio = read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0],\n",
       "       [   0,    0],\n",
       "       [   0,    0],\n",
       "       ...,\n",
       "       [-399, -115],\n",
       "       [-388, -111],\n",
       "       [-386, -105]], dtype=int16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14ca29b2a00>,\n",
       " <matplotlib.lines.Line2D at 0x14ca3291400>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.plot(audio[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100263-2-0-143.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100263-2-0-161.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100263-2-0-3.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100263-2-0-36.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100648-1-0-0.wav</td>\n",
       "      <td>100648</td>\n",
       "      <td>4.823402</td>\n",
       "      <td>5.471927</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID      start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032   0.000000   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263  58.500000  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263  60.500000  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263  63.000000  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263  68.500000  72.500000         1     5        2   \n",
       "5  100263-2-0-143.wav  100263  71.500000  75.500000         1     5        2   \n",
       "6  100263-2-0-161.wav  100263  80.500000  84.500000         1     5        2   \n",
       "7    100263-2-0-3.wav  100263   1.500000   5.500000         1     5        2   \n",
       "8   100263-2-0-36.wav  100263  18.000000  22.000000         1     5        2   \n",
       "9    100648-1-0-0.wav  100648   4.823402   5.471927         2    10        1   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  \n",
       "5  children_playing  \n",
       "6  children_playing  \n",
       "7  children_playing  \n",
       "8  children_playing  \n",
       "9          car_horn  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata=pd.read_csv('UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog_bark            1000\n",
       "children_playing    1000\n",
       "air_conditioner     1000\n",
       "street_music        1000\n",
       "engine_idling       1000\n",
       "jackhammer          1000\n",
       "drilling            1000\n",
       "siren                929\n",
       "car_horn             429\n",
       "gun_shot             374\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check whether the dataset is imbalanced\n",
    "metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now comes the preprocessing part**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features using Mel-Frequency Cepstral Coefficients(MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.7041687e+02, -5.0580988e+02, -4.4661014e+02, ...,\n",
       "        -4.8128598e+02, -4.7245877e+02, -4.6424646e+02],\n",
       "       [ 3.2360138e+01,  9.3758621e+01,  1.4270895e+02, ...,\n",
       "         1.0403226e+02,  1.1474959e+02,  1.1683490e+02],\n",
       "       [ 2.6088993e+01,  3.4996731e+01,  4.0650402e+01, ...,\n",
       "         1.1899885e+01,  1.7138016e+01,  1.3884659e+01],\n",
       "       ...,\n",
       "       [ 5.9291804e-01,  2.7261610e+00, -2.6107316e+00, ...,\n",
       "        -2.7341051e+00, -3.7578499e+00, -5.8995886e+00],\n",
       "       [-1.3069375e+00, -2.1206574e+00, -9.1924280e-01, ...,\n",
       "        -1.2278645e+00, -3.3723919e+00, -4.1921663e+00],\n",
       "       [-2.0638509e+00, -2.9027894e+00,  2.7276498e-01, ...,\n",
       "        -1.3872089e+00, -3.3255339e+00, -8.4046578e-01]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a feature extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(file):\n",
    "    librosa.load(file, res_type='kaiser_fast')\n",
    "    mfcc_features = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\n",
    "    mfcc_scaled_features =np.mean(mfcc_features.T,axis=0)\n",
    "\n",
    "    return mfcc_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.9728604e+02,  1.4199390e+02,  1.0424644e+01, -9.8004112e+00,\n",
       "       -8.0245886e+00,  7.5154042e+00, -1.0164132e+01, -7.2393765e+00,\n",
       "       -3.5004098e+00, -5.7000537e+00, -2.3489676e+00, -3.5407817e+00,\n",
       "        5.2213383e+00,  9.1667747e+00,  9.7039452e+00,  1.2727837e+01,\n",
       "        2.4204013e+00, -1.0693447e-01,  1.9542018e+00,  2.3954169e-01,\n",
       "        1.1634955e-01, -2.0959182e+00, -4.3151698e+00, -7.4520910e-01,\n",
       "        4.2331934e-01,  1.5813076e+00,  1.4602641e+00,  2.6433935e+00,\n",
       "        4.3686604e+00,  1.9432870e+00,  2.9634244e-03,  4.8470337e-02,\n",
       "        1.9078921e+00,  1.1821151e+00,  1.4464529e-01, -8.8062823e-01,\n",
       "       -2.5595131e+00, -1.7090167e+00, -1.3550457e+00,  3.8210499e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\rohan\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=40\n",
      "  return f(*args, **kwargs)\n",
      "147it [00:12, 11.93it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "audio_dataset_path = 'UrbanSound8K/UrbanSound8K/audio/'\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=feature_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the extracted features from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-397.28604, 141.9939, 10.424644, -9.800411, -...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[450.72662, -21.561567, 12.0586605, -0.4794998...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[481.85965, -1.5298109, 0.6514032, -1.6475505,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[490.1246, -0.18750022, 0.019547466, -0.025050...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[491.94803, -0.03681645, -0.012096148, -0.0235...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-397.28604, 141.9939, 10.424644, -9.800411, -...          dog_bark\n",
       "1  [450.72662, -21.561567, 12.0586605, -0.4794998...  children_playing\n",
       "2  [481.85965, -1.5298109, 0.6514032, -1.6475505,...  children_playing\n",
       "3  [490.1246, -0.18750022, 0.019547466, -0.025050...  children_playing\n",
       "4  [491.94803, -0.03681645, -0.012096148, -0.0235...  children_playing"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8732, 40), (8732,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to change categorical variables of y into numerical variables using get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to perform train test split on our audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6985, 40), (1747, 40), (6985, 10), (1747, 10))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 69850)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# Now since all the preprocessing is done its time to create out model for the audio processing data\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the number of classes we have\n",
    "num_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               4100      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 45,410\n",
      "Trainable params: 45,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Jupyter projects\\Audio-classification-using-deep-learning\\Audio Classification Data Preprocessing And Model Creation.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model2 \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model2\u001b[39m.\u001b[39madd(Dense(\u001b[39m100\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model2\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.5\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model2\u001b[39m.\u001b[39madd(Dense(\u001b[39m200\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(200, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "mode2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for model1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1144 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.26980, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2566 - accuracy: 0.1141 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.26980 to 2.26976, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1101 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.26976\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1160 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.26976\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2562 - accuracy: 0.1140 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.26976\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1132 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.26976\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2560 - accuracy: 0.1104 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.26976\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1089 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.26976\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1135 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.26976\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1155 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.26976\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1160 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.26976\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2565 - accuracy: 0.1137 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.26976\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1108 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.26976\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1132 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.26976\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1071 - val_loss: 2.2697 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.26976 to 2.26970, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1142 - val_loss: 2.2700 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.26970\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1147 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.26970\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2561 - accuracy: 0.1158 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.26970\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2565 - accuracy: 0.1114 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.26970\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1131 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.26970\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1137 - val_loss: 2.2699 - val_accuracy: 0.1070\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.26970\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2562 - accuracy: 0.1131 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.26970\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1112 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.26970\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2561 - accuracy: 0.1131 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.26970\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1171 - val_loss: 2.2697 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.26970\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1105 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.26970\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1104 - val_loss: 2.2698 - val_accuracy: 0.1070\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.26970\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.1165 - val_loss: 2.2697 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.26970\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1137 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.26970\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1137 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.26970\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1141 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.26970\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1115 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.26970\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2562 - accuracy: 0.1127 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.26970\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1135 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.26970\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1150 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.26970\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1127 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.26970\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1144 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.26970\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1151 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.26970\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1130 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.26970\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1120 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.26970\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1111 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.26970\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1158 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.26970\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1145 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.26970\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1145 - val_loss: 2.2700 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.26970\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1161 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.26970\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1118 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.26970\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1152 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.26970\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1141 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.26970\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1125 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.26970\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1127 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.26970\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1102 - val_loss: 2.2697 - val_accuracy: 0.1070\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.26970\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1122 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.26970\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1097 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.26970\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1148 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.26970\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2557 - accuracy: 0.1160 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.26970\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1127 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.26970\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1158 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.26970\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2573 - accuracy: 0.1171 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.26970\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1132 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.26970\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1125 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.26970\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1121 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.26970\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1170 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.26970\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1147 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.26970\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1171 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.26970\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.1097 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.26970\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1135 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.26970\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1161 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.26970\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.1120 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.26970\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1121 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.26970\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1135 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.26970\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1114 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.26970\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1115 - val_loss: 2.2700 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.26970\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1115 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.26970\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1164 - val_loss: 2.2697 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.26970\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1131 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.26970\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1098 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.26970\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1142 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.26970\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1101 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.26970\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1160 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.26970\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1141 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.26970\n",
      "Epoch 81/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1158 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.26970\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.1171 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.26970\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1151 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.26970\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2582 - accuracy: 0.1127 - val_loss: 2.2700 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.26970\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1125 - val_loss: 2.2699 - val_accuracy: 0.1070\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.26970\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1142 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.26970\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1138 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.26970\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1105 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.26970\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1171 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.26970\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2561 - accuracy: 0.1121 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.26970\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1162 - val_loss: 2.2698 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.26970\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1118 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.26970\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.1110 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.26970\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1072 - val_loss: 2.2697 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00094: val_loss improved from 2.26970 to 2.26969, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1079 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.26969\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2558 - accuracy: 0.1130 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.26969\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 2.2560 - accuracy: 0.1142 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.26969\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2561 - accuracy: 0.1155 - val_loss: 2.2698 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.26969\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.1138 - val_loss: 2.2699 - val_accuracy: 0.1048\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.26969\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 2.2558 - accuracy: 0.1145 - val_loss: 2.2699 - val_accuracy: 0.1042\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.26969\n",
      "Training completed in time:  0:00:32.243015\n"
     ]
    }
   ],
   "source": [
    "## Training my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10417859256267548\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for model2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Jupyter projects\\Audio-classification-using-deep-learning\\Audio Classification Data Preprocessing And Model Creation.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m checkpointer \u001b[39m=\u001b[39m ModelCheckpoint(filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msaved_models/audio_classification.hdf5\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model2\u001b[39m.\u001b[39mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39mnum_batch_size, epochs\u001b[39m=\u001b[39mnum_epochs, validation_data\u001b[39m=\u001b[39m(X_test, y_test), callbacks\u001b[39m=\u001b[39m[checkpointer], verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Audio-classification-using-deep-learning/Audio%20Classification%20Data%20Preprocessing%20And%20Model%20Creation.ipynb#X56sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed in time: \u001b[39m\u001b[39m\"\u001b[39m, duration)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "## Training my model2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model2.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=model2.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae9c6dbfe98c233b552ade8845b36d543790a6937789b97776baad4875d76556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
